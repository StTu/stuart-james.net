<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stuart James&#39; Journal</title>
    <description>Stuart James&#39; journal, a simple little place for me to scribble down my thoughts and help my terrible memory</description>
    <link>http://stuart-james.net/</link>
    <atom:link href="http://stuart-james.net/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 29 Aug 2016 23:04:16 +0100</pubDate>
    <lastBuildDate>Mon, 29 Aug 2016 23:04:16 +0100</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Transferring old book image style to real world photos</title>
        <description>&lt;p&gt;
Neural Style Transfer [Gatys&#39;15] has become a popular area of research and with public  applications such as Prisma, but earlier this week I wanted to answer the question does it really work for understanding the larger style. In contrast to [Wang&#39;13] where they learnt an artistic style how does it compare. The British Library Flickr 1m+ dataset [BL&#39;13] provides an interesting application of this. Where there is an inherent style for transferring. 
&lt;/p&gt;
&lt;!--more--&gt;

&lt;p&gt;
So having read the papers around this previously I was fairly sure it would not transfer very well, but on the chance that local statistics can enforce something coherent it was worth running (and also gave me a chance to play with such networks). So by taking a few examples these are the best results from transferring from the BL&#39;13 to the &lt;a href=&quot;https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html&quot;&gt;Berkeley Segmentation Dataset (BSDS500) [BSDS500&#39;11]&lt;/a&gt;
&lt;/p&gt;

&lt;img style=&quot;width:100%&quot; src=&quot;/images/blog/2016-08-26-style-collage.png&quot;&gt;


&lt;p&gt;
    These results were achieved using the Texture Nets method [Ulyanov&#39;16] trained on a singular example and the most visually appealing results displayed after playing with the Texture / Style weights as well as the chosen example. Code available on GitHub &lt;a href=&quot;https://github.com/DmitryUlyanov/texture_nets&quot;&gt;https://github.com/DmitryUlyanov/texture_nets&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;img style=&quot;max-width:33%; &quot; src=&quot;/images/blog/2016-08-26-style-example.jpg&quot;&gt;
&lt;img style=&quot;max-width:33%; &quot; src=&quot;/images/blog/2016-08-26-style-bsd500.jpg&quot;&gt;
&lt;img style=&quot;max-width:33%; &quot; src=&quot;/images/blog/2016-08-26-style-bsd500-stylaized.jpg&quot;&gt;
&lt;/p&gt;
&lt;p&gt;
    What is quite interesting is if you look at these from a distance they look plausible but it isn&#39;t till you look at one adjacent to another or zoom in you realise that these aren&#39;t actually the same style. Logical hatching patterns to describe shadows or depth are ignored, or in the cases where they are present they don&#39;t make sense. 
&lt;/p&gt;

&lt;p&gt;
As a mini-conclusion, style-transfer although gaining a lot of hype has still a long way to being accurately reproduce general artistic style. Still the results are interesting and if you aren&#39;t looking for exact replication then it is visually appealing. It must be bared in mind that the British Library dataset is challenging, where the style has been evolved for human understanding over millenniums. A problem to keep working on, possibly guided by transfer learning.
&lt;/p&gt;


&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;[Gatys&#39;15] Leon A Gatys and Alexander S Ecker and Matthias Bethge. &quot;A Neural Algorithm of Artistic Style&quot;. Arxiv (http://arxiv.org/abs/1508.06576). 2015.
&lt;br /&gt;
[Wang&#39;13] T Wang and J Collomosse and D Greig and A Hunter. &quot;Learnable Stroke Models for Example-based Portrait Painting&quot;. Proc. British Machine Vision Conference (BMVC). 2013.
&lt;br /&gt;
[BL&#39;13] British Library Flickr. https://www.flickr.com/photos/britishlibrary/. 2013.
&lt;br /&gt;
[BSDS500&#39;11] Berkeley Segmentation Dataset. https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html#bsds500. 2011
&lt;br /&gt;
[Ulyanov&#39;16] Dmitry Ulyanov and Vadim Lebedev and Andrea Vedaldi and Victor Lempitsky. &quot;Texture Networks: Feed-forward Synthesis of Textures and Stylized Images. Arxiv (http://arxiv.org/abs/1603.03417). 2016.
&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Aug 2016 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/2016/08/26/british-library-dataset-style-transfer.html</link>
        <guid isPermaLink="true">http://stuart-james.net/2016/08/26/british-library-dataset-style-transfer.html</guid>
        
        
      </item>
    
      <item>
        <title>Cheddar Gorge Relish Run</title>
        <description>&lt;p&gt;
As I continue my crazy running activities I take another shot at the Relish Run Cheddar Gorge half marathon. A great route that takes you up over the hills surrounding Cheddar and past the trig point with views over Weston-super-Mare, my home town. 
&lt;/p&gt;
&lt;!--more--&gt;

&lt;p&gt;
So this is a hard run and this years challenge was water, to note last years was sun. So I&#39;ve tried this course into polar conditions (for the UK), it seems that water is more challenging. This was not aided by me getting lost and taking another guy with me.
&lt;/p&gt;


&lt;img style=&quot;width:100%&quot; src=&quot;/images/blog/2016-08-21-cheddar-route-profile.jpg&quot;&gt;
&lt;p&gt;Well another year another race attempt, hoping for better next year!&lt;/p&gt;</description>
        <pubDate>Sun, 21 Aug 2016 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/2016/08/21/cheddar-gorge-relish-run.html</link>
        <guid isPermaLink="true">http://stuart-james.net/2016/08/21/cheddar-gorge-relish-run.html</guid>
        
        
      </item>
    
      <item>
        <title>San Francisco Second Half Marathon</title>
        <description>&lt;p&gt;
I&#39;ve been running for a few years now, doing as many runs as I can a year. Granted my training routine isn&#39;t very vigorous, but just doing the run is great fun. After attending SIGGRAPH this year, I passed through San Francisco on my drive up the coast. As any avid runner would, if there a race you can do you do. So I ran the San Francisco Second Half Marathon!
&lt;/p&gt;
&lt;!--more--&gt;

&lt;img style=&quot;width:100%;&quot; src=&quot;/images/blog/2016-07-31-san-fran-run-start.jpg&quot;&gt;

&lt;p&gt;
    I wish I could say this went well, but this is a challenging route, but was so good to run. I hope in future I can come back and do the marathon in full, one of the unique things about this marathon it isn&#39;t just two laps of the half. Therefore by doing the marathon you get great views of the golden gate bridge and through the centre of the city. 
&lt;/p&gt;

&lt;img style=&quot;width:100%;&quot; src=&quot;/images/blog/2016-07-31-san-fran-run-profile.jpg&quot;&gt;</description>
        <pubDate>Sun, 31 Jul 2016 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/2016/07/31/San-Francisco-Second-Half.html</link>
        <guid isPermaLink="true">http://stuart-james.net/2016/07/31/San-Francisco-Second-Half.html</guid>
        
        
      </item>
    
      <item>
        <title>Getting back to books</title>
        <description>&lt;p&gt;
When I was younger I never got into reading novels sadly. I have tried throughout my adult life to try to get into them, but sadly it is always a challenge to stick with them. Often I find the demands of my work a massive distraction result in me not wanting to sit and read more. I dabbled into Audiobooks that I recommend massively, but loosing my commute broke this. So I&#39;m going to try listen while cycling to work! 
&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;
Having spent a while now (6months) trying to finish &lt;a href=&quot;https://www.goodreads.com/book/show/22088245-npcs&quot;&gt;NPCs by Drew Hayes&lt;/a&gt; a great fun book. Sadly the inprogress nature of this preculudes me from being able to move onto the second in the book series. Therefore my book to try next is &lt;a href=&quot;https://www.goodreads.com/book/show/15701981-critical-failures&quot;&gt;Critical Failures by Rober Bevan&lt;/a&gt;. The audiobook at 8hrs should be an easy &#39;read&#39; in a week, lets see if I crash!
&lt;/p&gt;


&lt;div class=&quot;alert alert-info&quot; role=&quot;alert&quot;&gt;
&lt;strong&gt;27.06.2016 Update:&lt;/strong&gt; To give an update on this, cycling and audiobooks doesn&#39;t work, but not for the reasons I expected. Instead of providing an increased danger on the streets of London, I could barely hear the audio. It seems that spoken words are harder to hear than music, so listening while cycling in last week I struggled to keep up with events in the book. Pitty, more thinking of how I&#39;m going to get my book fix in.
&lt;/div&gt;
</description>
        <pubDate>Thu, 23 Jun 2016 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/2016/06/23/getting-back-to-books.html</link>
        <guid isPermaLink="true">http://stuart-james.net/2016/06/23/getting-back-to-books.html</guid>
        
        
      </item>
    
      <item>
        <title>29 going on 30</title>
        <description>A mini-celebration of reflection as I hit my 29th year floating around the sun. Although I may not of achieved everything I have set out todo in this decade I have a lot of achievements both career / academically and life through running etc.

So happy 29th to me!</description>
        <pubDate>Tue, 14 Jun 2016 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/2016/06/14/twenty-nine-going-on-thirty.html</link>
        <guid isPermaLink="true">http://stuart-james.net/2016/06/14/twenty-nine-going-on-thirty.html</guid>
        
        
      </item>
    
      <item>
        <title>Working with the British Library Dataset</title>
        <description>&lt;p&gt;Earlier this year &lt;a href=&quot;http://tim.weyri.ch&quot;&gt;Tim Weyrich&lt;/a&gt; directed me onto a dataset published by the British Library and since then my research has focused heavily around this. Within Computer Vision it is unusual to get a large dataset not skewed to achieve a specific research goal. Sometimes datasets can be repurposed, but this is requires extensive effort to get the data in its rawest form.&lt;/p&gt;

&lt;p&gt;The British Library dataset is a quite literally a &quot;dump&quot; of all the unknown to OCR elements from the book scanning performed by Microsoft. Therefore is not just a collection of line art imagery, but also of elaborate charachters or section embroidery.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;I intend to post more on working with this dataset as time goes on, but for now there is a github which contains the directory of images:
&lt;a href=&quot;https://github.com/BL-Labs/imagedirectory&quot;&gt;Directory of Images (Github)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And what is the main repository on Flickr:&lt;a href=&quot;https://www.flickr.com/photos/britishlibrary/&quot;&gt;Flickr Repository&lt;/a&gt;

</description>
        <pubDate>Wed, 20 Apr 2016 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/2016/04/20/working-with-british-library-dataset.html</link>
        <guid isPermaLink="true">http://stuart-james.net/2016/04/20/working-with-british-library-dataset.html</guid>
        
        
      </item>
    
      <item>
        <title>Moving to Jekyll</title>
        <description>&lt;p&gt;After many (and many) years of using dynamic CMS, I have decided to take the plunge and move to an old school methodology -- static websites. Well when I say &quot;static&quot;, I mean offline generated site, I&#39;m not crazy after all. I don&#39;t want to spend the next years of my life changing tiny global settings on my website, but more I just don&#39;t see the need to actually have a heavy database driven site running all the time, when to be honest I don&#39;t often update the site.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;
So why did I stop updating, well that answer comes in two parts. Firstly I experienced some problems with the CMS I was using, naughty Mojo Portal! But this just limited my ability to post. The second and main reason is that I lacked time and I&#39;m not saying I have more time now, but I just didn&#39;t want to spend the small amount of time I have allocated to my website on repairing the website.
&lt;/p&gt;


&lt;p&gt;
So the the solution Jeykll! A perl based offline static site generator. I must admit the choice of this wasn&#39;t exactly done on an extensive search and making a grounded decision, but instead I used github and it seemed a logical extension. That is not to say I did no search, I glanced, but there didn&#39;t seem a big differentiator other than language. So Perl and Jekyll it is, why not learn a language at the same time after all, so even if this all fails I&#39;ve learned something generalisable. 
&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Apr 2016 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/2016/04/14/moving-to-jekyll.html</link>
        <guid isPermaLink="true">http://stuart-james.net/2016/04/14/moving-to-jekyll.html</guid>
        
        
      </item>
    
      <item>
        <title>Why I am a resolution junky</title>
        <description>&lt;p&gt;I have been coding for many years now (scarily &amp;gt; 15&amp;nbsp;years), I have always aimed to get higher and higher resolution screens or alternatively multiple screens. Sadly as mentioned in an earlier blog post the computer I am using at the moment is a desktop replacement&amp;nbsp;laptop with a dying screen so I purchased a 27&quot; 2560x1440, not quite my laptops 3200x1800 but still pretty reasonable and great for late night coding! The above is an example of me geeking out, library coding and demo/test rig&amp;nbsp;coding in parallel&lt;/p&gt;</description>
        <pubDate>Mon, 20 Oct 2014 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/tech/2014/10/20/why-i-am-a-resolution-junky.html</link>
        <guid isPermaLink="true">http://stuart-james.net/tech/2014/10/20/why-i-am-a-resolution-junky.html</guid>
        
        
        <category>tech</category>
        
      </item>
    
      <item>
        <title>New Toys Sept’14 Part 1 of 2 – BenQ BL2710PT 27” WQHD</title>
        <description>&lt;p&gt;After having my beast of a desktop replacement laptop (ASUS n73sv) for a few years now the screen has become a bit temperamental. This and the fact I am using Lenovo Yoga2 Pro ultra book more and more (due to portability/screen res) has resulted in a void in powerful home processing.&lt;/p&gt;

&lt;p&gt;Therefore I felt to solve this problem I would get a beefy screen to allow me to work more comfortably at home and in theory write more of my thesis (instead of this blog… shh). I wanted something above the 1920x1080 resolution, since I know that two screens isn’t an option used in conjunction with the old ASUS. Two competitors came to the fore the BenQ BL2710PT and a Dell, oddly the Dell had very bad reversed when used over HDMI and you have to do a lot of hard work to get over the locked down 1080p. So I opted for the BenQ and am very happy with the results.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2&gt;The BenQ BL2710PT 27”&lt;/h2&gt;

&lt;p&gt;No one would ever say this is a beautiful monitor, but they would say it is very functional. It does what it needs and has wonderful colour and good true-blacks. Although my graphics card wasn’t over the moon about running at 1440 after a little convincing was happy to run 1440 @ 55Hz (don’t ask why not 60, I couldn’t coax it into getting up to that). I read some reviews of the equivalent Dell that you can only get 30Hz over HDMI with some converters and that it is fine if you increase the mouse rate, that is complete rubbish it feels laggy at 30Hz for a production machine it is simply not good enough.&lt;/p&gt;

&lt;p&gt;The one criticism I have is the inbuilt speakers, they are pretty terrible (not that I have had much good experience with in screen-speakers). You have to remember this is marketed as a CAD monitor so you wouldn’t expect this to be a high interest feature. The addition of 2 USB3 ports on this side of the screen is a nice touch very useful extending out your storage options.&lt;/p&gt;

&lt;h2&gt;Some kind of conclusion&lt;/h2&gt;

&lt;p&gt;It is hard to have a decisive exciting conclusion from a screen, it does what it is supposed to. Would I opt for two monitors over this definitely, if you don’t have that as an option or you want to watch movies in bed then this is a great screen for you to get that extra screen estate. The only real hurdle to this screen is price, at £400 it is steep especially when you think a 1080 screen will set you back £150, a couple of those comes in a lot cheaper for more screen real-estate.&lt;/p&gt;

&lt;p&gt;I do know there are some entry level UltraHD( 3840x2160 ) screens often these are locked to 30Hz. Ignoring that issue, I simply didn’t need it my GPU was being pushed to max-resolution poor little GeForce 560m so I opted to save the money and probably get a better UHD screen later when I get something to power it.&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Sep 2014 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/tech/2014/09/26/new-toys-sept-14-part-1-of-2-benq-bl2710pt-27-wqhd.html</link>
        <guid isPermaLink="true">http://stuart-james.net/tech/2014/09/26/new-toys-sept-14-part-1-of-2-benq-bl2710pt-27-wqhd.html</guid>
        
        
        <category>tech</category>
        
      </item>
    
      <item>
        <title>libSVM linear kernel normalisation</title>
        <description>&lt;p&gt;I have used a variety of tools for binary, multiclass and even incremental SVM problems, today I found something quite nice in binary case for &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libSVM&lt;/a&gt;, although potentially a source of confusion.&lt;/p&gt;&lt;p&gt;It is common in machine learning to apply a sigmoid function to normalise the boundaries of a problem, this can by empirically defining the upper and lower bound or through experimentation. Within &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libSVM&lt;/a&gt; they do this through experimentation, that is great to save you some time. The only thing to remember is it means through the use of random and cross validation with small sets of data, you are likely to get different results on each run.&lt;/p&gt;&lt;p&gt;So the function to consider is this:&lt;/p&gt;

``` cpp

// Cross-validation decision values for probability estimates
static void svm_binary_svc_probability(
	const svm_problem *prob, const svm_parameter *param,
	double Cp, double Cn, double&amp;amp; probA, double&amp;amp; probB)
{
	int i;
	int nr_fold = 5;
	int *perm = Malloc(int,prob-&amp;gt;l);
	double *dec_values = Malloc(double,prob-&amp;gt;l);

	// random shuffle
	for(i=0;i&amp;lt;prob-&amp;gt;l;i++) perm[i]=i;
	for(i=0;i&amp;lt;prob-&amp;gt;l;i++)
	{
		int j = i+rand()%(prob-&amp;gt;l-i);
		swap(perm[i],perm[j]);
	}
	for(i=0;i&amp;lt;nr_fold;i++)
	{
		int begin = i*prob-&amp;gt;l/nr_fold;
		int end = (i+1)*prob-&amp;gt;l/nr_fold;
		int j,k;
		struct svm_problem subprob;

		subprob.l = prob-&amp;gt;l-(end-begin);
		subprob.x = Malloc(struct svm_node*,subprob.l);
		subprob.y = Malloc(double,subprob.l);
			
		k=0;
		for(j=0;j&amp;lt;begin;j++)
		{
			subprob.x[k] = prob-&amp;gt;x[perm[j]];
			subprob.y[k] = prob-&amp;gt;y[perm[j]];
			++k;
		}
		for(j=end;j&amp;lt;prob-&amp;gt;l;j++)
		{
			subprob.x[k] = prob-&amp;gt;x[perm[j]];
			subprob.y[k] = prob-&amp;gt;y[perm[j]];
			++k;
		}
		int p_count=0,n_count=0;
		for(j=0;j&amp;lt;k;j++)
			if(subprob.y[j]&amp;gt;0)
				p_count++;
			else
				n_count++;

		if(p_count==0 &amp;amp;&amp;amp; n_count==0)
			for(j=begin;j&amp;lt;end;j++)
				dec_values[perm[j]] = 0;
		else if(p_count &amp;gt; 0 &amp;amp;&amp;amp; n_count == 0)
			for(j=begin;j&amp;lt;end;j++)
				dec_values[perm[j]] = 1;
		else if(p_count == 0 &amp;amp;&amp;amp; n_count &amp;gt; 0)
			for(j=begin;j&amp;lt;end;j++)
				dec_values[perm[j]] = -1;
		else
		{
			svm_parameter subparam = *param;
			subparam.probability=0;
			subparam.C=1.0;
			subparam.nr_weight=2;
			subparam.weight_label = Malloc(int,2);
			subparam.weight = Malloc(double,2);
			subparam.weight_label[0]=+1;
			subparam.weight_label[1]=-1;
			subparam.weight[0]=Cp;
			subparam.weight[1]=Cn;
			struct svm_model *submodel = svm_train(&amp;amp;subprob,&amp;amp;subparam);
			for(j=begin;j&amp;lt;end;j++)
			{
				svm_predict_values(submodel,prob-&amp;gt;x[perm[j]],&amp;amp;(dec_values[perm[j]])); 
				// ensure +1 -1 order; reason not using CV subroutine
				dec_values[perm[j]] *= submodel-&amp;gt;label[0];
			}		
			svm_free_and_destroy_model(&amp;amp;submodel);
			svm_destroy_param(&amp;amp;subparam);
		}
		free(subprob.x);
		free(subprob.y);
	}		
	sigmoid_train(prob-&amp;gt;l,dec_values,prob-&amp;gt;y,probA,probB);
	free(dec_values);
	free(perm);
}
```

So if you have few numbers of samples, that is the case in some circumstances then the cross validation is where you hit problems. Of course you can simply re-implement it yourself or you can add a few lines to stop cross validation if the number of samples is too few.&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Not the most elegant of code, but for the moment it will do. I choose to completely seperate the two steps as opposed to multiple if’s&lt;/p&gt;&lt;pre class=&quot;brush: c++; toolbar: false&quot;&gt;static void svm_binary_svc_probability(
``` cpp
	const svm_problem *prob, const svm_parameter *param,
	double Cp, double Cn, double&amp;amp; probA, double&amp;amp; probB)
{
	int i;
	int nr_fold = 5;
	int *perm = Malloc(int,prob-&amp;gt;l);
	double *dec_values = Malloc(double,prob-&amp;gt;l);

	// random shuffle
	for(i=0;i&amp;lt;prob-&amp;gt;l;i++) perm[i]=i;
	for(i=0;i&amp;lt;prob-&amp;gt;l;i++)
	{
		int j = i+rand()%(prob-&amp;gt;l-i);
		swap(perm[i],perm[j]);
	}
	if (prob-&amp;gt;l &amp;lt; (5*nr_fold)){
		int begin = 0;
		int end = prob-&amp;gt;l;

		int j,k;

		struct svm_problem subprob;

		subprob.l = prob-&amp;gt;l;
		subprob.x = Malloc(struct svm_node*,subprob.l);
		subprob.y = Malloc(double,subprob.l);
			
		k=0;
		for(j=0;j&amp;lt;prob-&amp;gt;l;j++)
		{
			subprob.x[k] = prob-&amp;gt;x[perm[j]];
			subprob.y[k] = prob-&amp;gt;y[perm[j]];
			++k;
		}


		int p_count=0,n_count=0;
		for(j=0;j&amp;lt;k;j++)
			if(prob-&amp;gt;y[j]&amp;gt;0)
				p_count++;
			else
				n_count++;

		if(p_count==0 &amp;amp;&amp;amp; n_count==0)
			for(j=begin;j&amp;lt;end;j++)
				dec_values[perm[j]] = 0;
		else if(p_count &amp;gt; 0 &amp;amp;&amp;amp; n_count == 0)
			for(j=begin;j&amp;lt;end;j++)
				dec_values[perm[j]] = 1;
		else if(p_count == 0 &amp;amp;&amp;amp; n_count &amp;gt; 0)
			for(j=begin;j&amp;lt;end;j++)
				dec_values[perm[j]] = -1;
		else
		{
			svm_parameter subparam = *param;
			subparam.probability=0;
			subparam.C=1.0;
			subparam.nr_weight=2;
			subparam.weight_label = Malloc(int,2);
			subparam.weight = Malloc(double,2);
			subparam.weight_label[0]=+1;
			subparam.weight_label[1]=-1;
			subparam.weight[0]=Cp;
			subparam.weight[1]=Cn;
			struct svm_model *submodel = svm_train(&amp;amp;subprob,&amp;amp;subparam);
			for(j=begin;j&amp;lt;end;j++)
			{
				svm_predict_values(submodel,prob-&amp;gt;x[perm[j]],&amp;amp;(dec_values[perm[j]])); 
				// ensure +1 -1 order; reason not using CV subroutine
				dec_values[perm[j]] *= submodel-&amp;gt;label[0];
			}		
			svm_free_and_destroy_model(&amp;amp;submodel);
			svm_destroy_param(&amp;amp;subparam);
		}
		free(subprob.x);
		free(subprob.y);
	}else{
		for(i=0;i&amp;lt;nr_fold;i++)
		{
			int begin = i*prob-&amp;gt;l/nr_fold;
			int end = (i+1)*prob-&amp;gt;l/nr_fold;
			if (nr_fold == 1){
				begin = 0 ;
				end = prob-&amp;gt;l;
			}

			int j,k;
			struct svm_problem subprob;

			subprob.l = prob-&amp;gt;l-(end-begin);
			subprob.x = Malloc(struct svm_node*,subprob.l);
			subprob.y = Malloc(double,subprob.l);
			
			k=0;
			for(j=0;j&amp;lt;begin;j++)
			{
				subprob.x[k] = prob-&amp;gt;x[perm[j]];
				subprob.y[k] = prob-&amp;gt;y[perm[j]];
				++k;
			}
			for(j=end;j&amp;lt;prob-&amp;gt;l;j++)
			{
				subprob.x[k] = prob-&amp;gt;x[perm[j]];
				subprob.y[k] = prob-&amp;gt;y[perm[j]];
				++k;
			}
			int p_count=0,n_count=0;
			for(j=0;j&amp;lt;k;j++)
				if(subprob.y[j]&amp;gt;0)
					p_count++;
				else
					n_count++;

			if(p_count==0 &amp;amp;&amp;amp; n_count==0)
				for(j=begin;j&amp;lt;end;j++)
					dec_values[perm[j]] = 0;
			else if(p_count &amp;gt; 0 &amp;amp;&amp;amp; n_count == 0)
				for(j=begin;j&amp;lt;end;j++)
					dec_values[perm[j]] = 1;
			else if(p_count == 0 &amp;amp;&amp;amp; n_count &amp;gt; 0)
				for(j=begin;j&amp;lt;end;j++)
					dec_values[perm[j]] = -1;
			else
			{
				svm_parameter subparam = *param;
				subparam.probability=0;
				subparam.C=1.0;
				subparam.nr_weight=2;
				subparam.weight_label = Malloc(int,2);
				subparam.weight = Malloc(double,2);
				subparam.weight_label[0]=+1;
				subparam.weight_label[1]=-1;
				subparam.weight[0]=Cp;
				subparam.weight[1]=Cn;
				struct svm_model *submodel = svm_train(&amp;amp;subprob,&amp;amp;subparam);
				for(j=begin;j&amp;lt;end;j++)
				{
					svm_predict_values(submodel,prob-&amp;gt;x[perm[j]],&amp;amp;(dec_values[perm[j]])); 
					// ensure +1 -1 order; reason not using CV subroutine
					dec_values[perm[j]] *= submodel-&amp;gt;label[0];
				}		
				svm_free_and_destroy_model(&amp;amp;submodel);
				svm_destroy_param(&amp;amp;subparam);
			}
			free(subprob.x);
			free(subprob.y);
		}
	}
	sigmoid_train(prob-&amp;gt;l,dec_values,prob-&amp;gt;y,probA,probB);
	free(dec_values);
	free(perm);
}
```
&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;As with a lot of my code based posts, this is more for my memory than anything, but hopefully may help people unlock the secrets of &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libSVM&lt;/a&gt;.&lt;/p&gt;</description>
        <pubDate>Mon, 07 Jul 2014 00:00:00 +0100</pubDate>
        <link>http://stuart-james.net/2014/07/07/libsvm-linear-kernel-normalisation.html</link>
        <guid isPermaLink="true">http://stuart-james.net/2014/07/07/libsvm-linear-kernel-normalisation.html</guid>
        
        
      </item>
    
  </channel>
</rss>
